---
title: "2024.09 blog review"
date: 2024-08-31T17:40:57+08:00
draft: false
toc: false
description: blog review
tags: 
  - blog
---

# NetWork

## 工业级大规模RDMA技术杂谈 & 《RDMA杂谈》专栏索引
[《RDMA杂谈》专栏索引](https://zhuanlan.zhihu.com/p/164908617)
[工业级大规模RDMA技术杂谈](https://zhuanlan.zhihu.com/p/510323418)

非常好的资料，扫盲RDMA用

## Deploying User-space TCP at Cloud Scale with LUNA

[链接](https://www.usenix.org/conference/atc23/presentation/zhu-lingjun)
需要用户态tcp的原因
1. 存储集群内可以rdma，但计算集群和存储集群之间可能跨pod，RDMA默认流控为PFC，跨Pod通信，存在PFC风暴和死锁等隐患;且计算节点硬件可能不支持rdma
2. 内核tcp性能不达标（一次4KB的RTT大概需要50us，而EBS的SLO就是100us），不够scale，存在datacenter tax
3. 方便做RTC

Luna的几个特点
1. 作为一个libary，集成到用户进程中
2. share nothing，利用网卡多队列技术，按照核心划分流量。没有work steal这种复杂的机制。
3. 兼容内核TCP
4. 线程模型上，Run to Completion
5. 内存模型上，构建用户态内存分配器，实现全路径zero copy

Luna 4KB的RTT latency大概在20us（kernel 为50us）

## From Luna to Solar: The Evolutions of the Compute-to-Storage Networks in Alibaba Cloud
[链接](https://zhuanlan.zhihu.com/p/648322713)
Luna相比于Kernel，能节省CPU，但随着硬件的发展，它对CPU的 消耗还是不可忽视，比如说Luna需要吃掉4个core来跑满200Gbps网卡（Kernel需要12个）；再加上计算节点上的存储Agent（做路由，加解密，CRC等），会消耗客户机的不少CPU。
Solar就将协议栈和Agent Offload到了FPGA中（相当于DPU）。基于RoceV2,传出层协议为UDP，拥塞控制算法为HPCC

# Storage
## S3: A Scalable In-memory Skip-List Index for Key-Value Store
[链接](https://www.vldb.org/pvldb/vol12/p2183-zhang.pdf)

1. Skip-List顶层节点称为Guard Entry，限制其的总大小，使其能被L2 Cache住。
2. 其余节点称为Data Entry，一个Data Entry可以容纳若干kv，Data Entry内key无序，Data Entry间有序i（看论文的图就明白
3. Guard Entry按照线程来shard写入，第i个Guard Entry和i+1个Guard Entry之间的数据更新由一个线程负责

## CloudJump: Optimizing Cloud Database For Cloud Storage
[链接]([https://www.vldb.org/pvldb/vol12/p2183-zhang.pdf](https://www.vldb.org/pvldb/vol15/p3432-chen.pdf))

PolarDB在PolarFS上运行存储引擎，相比于Aurora给计算层定制一个log存储层，PolarFS更像是一个通用的云存储系统，文章主要提出了在云存储之上构建单机存储引擎的挑战和优化手段。
云存储跟本地SSD相比，latency更高，但吞吐、带宽和并行度都更高，所以在设计存储引擎的时候，需要利用其优点，避其短板
1. Aries-style的集中式日志导致io并行度很低，并将云存储的高latency直接暴露在前台写入中。优化成page partition log，并且每一路log都有多个writer，因为一个文件的多个chunk可能分布在不同存储节点，并行写文件也可以提高并行度。
2. io要分优先级，其中log的写入和page的读取是前台流量，优先级最高。log读取和脏页下刷优先级较低
3. pre-fetch的收益相比基于ssd要更大
4. 细粒度锁。因为io慢了，所以按理来说持有各种锁的时间也会更长，同步的开销也会更大。CloudJump用了影子页机制，加锁->内存里拷一份->直接解锁，而不是写完了再解锁。还实现了B-Link Tree，一次最多锁2个节点。
5. 
